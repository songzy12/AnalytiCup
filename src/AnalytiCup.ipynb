{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import some modules and set some paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from util import dump_submission\n",
    "from feature import get_feature, get_tokenizer\n",
    "from model import train_model\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load\n",
    "\n",
    "Then we read the input from the training files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en_train = pd.read_csv(english_train_path, sep='\\t', names=['en0', 'es0', 'en1', 'es1', 'label'])\n",
    "\n",
    "df_es_train = pd.read_csv(spanish_train_path, sep='\\t', names=['es0', 'en0', 'es1', 'en1', 'label'])\n",
    "df_es2en = pd.read_csv(unlabel_spanish_train_path, sep='\\t', names=['es', 'en'])\n",
    "df_test = pd.read_csv(test_path, sep='\\t', names=['es0', 'es1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1400, 5000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_es_train), len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer([df_es_train['es0'], df_es_train['es1'], df_test['es0'], df_test['es1']])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec\n",
    "\n",
    "Then we load the pretrained embedding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'985667 300\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "f = open(es_vec_path, encoding=\"utf8\")\n",
    "f.readline()\n",
    "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.13075  , -0.087659 , -0.11427  , -0.020641 ,  0.11753  ,\n",
       "        0.19687  ,  0.054257 , -0.0028717,  0.062278 , -0.10023  ,\n",
       "       -0.050123 , -0.026275 , -0.057605 , -0.13072  ,  0.10147  ,\n",
       "        0.15849  ,  0.095493 ,  0.051555 ,  0.015874 , -0.046374 ,\n",
       "        0.098467 ,  0.034867 ,  0.039933 , -0.1208   ,  0.065478 ,\n",
       "       -0.0098815, -0.13914  , -0.043732 , -0.015622 ,  0.05665  ,\n",
       "       -0.01476  , -0.0054753, -0.047127 , -0.21595  , -0.015154 ,\n",
       "       -0.0034798,  0.058253 ,  0.036444 , -0.25157  ,  0.060459 ,\n",
       "        0.23842  ,  0.017983 ,  0.10673  , -0.15889  ,  0.23043  ,\n",
       "       -0.078636 ,  0.075394 , -0.18431  , -0.31417  ,  0.084773 ,\n",
       "       -0.14912  ,  0.036904 , -0.1144   ,  0.025056 ,  0.058607 ,\n",
       "        0.059822 , -0.17929  ,  0.028468 ,  0.16728  , -0.020946 ,\n",
       "        0.019714 ,  0.0083937,  0.032227 ,  0.013204 ,  0.06393  ,\n",
       "       -0.19616  , -0.043487 ,  0.10124  , -0.032762 ,  0.17206  ,\n",
       "       -0.062339 , -0.10172  , -0.31708  ,  0.079012 , -0.1232   ,\n",
       "       -0.15504  , -0.084187 , -0.099777 ,  0.16626  ,  0.086791 ,\n",
       "        0.001035 ,  0.10478  ,  0.12913  , -0.0026416,  0.061668 ,\n",
       "        0.10004  , -0.073838 ,  0.167    ,  0.10342  , -0.05263  ,\n",
       "        0.20125  ,  0.23046  ,  0.043589 ,  0.19497  , -0.0093385,\n",
       "       -0.042631 , -0.17599  , -0.15208  ,  0.23261  , -0.10049  ,\n",
       "        0.096678 , -0.030501 ,  0.060627 , -0.27119  , -0.11177  ,\n",
       "        0.26739  ,  0.205    , -0.13012  ,  0.051966 ,  0.18115  ,\n",
       "        0.04977  ,  0.069596 , -0.039885 ,  0.20009  , -0.13619  ,\n",
       "        0.28566  ,  0.16922  , -0.084435 , -0.25588  , -0.019324 ,\n",
       "        0.024354 ,  0.017741 ,  0.084594 , -0.15838  , -0.18982  ,\n",
       "       -0.14319  ,  0.017073 ,  0.03242  ,  0.21051  ,  0.14096  ,\n",
       "        0.076237 ,  0.033525 ,  0.0043038,  0.064959 , -0.037789 ,\n",
       "       -0.14099  ,  0.0075984, -0.11115  , -0.035141 ,  0.14261  ,\n",
       "        0.053846 , -0.057024 , -0.0094945,  0.10681  ,  0.02614  ,\n",
       "        0.042217 , -0.070822 , -0.10298  ,  0.094206 ,  0.045829 ,\n",
       "        0.2296   , -0.10944  ,  0.10839  , -0.1162   , -0.062974 ,\n",
       "       -0.037453 , -0.041537 , -0.071237 , -0.086044 ,  0.061869 ,\n",
       "       -0.081216 ,  0.13235  ,  0.013349 ,  0.041143 ,  0.096013 ,\n",
       "       -0.14398  ,  0.050647 , -0.21338  , -0.05996  , -0.076753 ,\n",
       "        0.1126   ,  0.1712   ,  0.11117  ,  0.0094953, -0.14779  ,\n",
       "       -0.22474  , -0.063649 , -0.23653  , -0.014093 ,  0.13146  ,\n",
       "       -0.014384 , -0.10514  ,  0.012632 , -0.014102 ,  0.0011301,\n",
       "        0.0052141, -0.09386  ,  0.026303 , -0.19247  ,  0.15463  ,\n",
       "       -0.070266 , -0.08647  ,  0.032046 , -0.141    , -0.11656  ,\n",
       "       -0.11     ,  0.13072  ,  0.075912 , -0.050148 ,  0.15808  ,\n",
       "       -0.011774 ,  0.064582 , -0.17485  , -0.055014 , -0.21166  ,\n",
       "        0.23468  ,  0.27994  , -0.0047165, -0.058851 , -0.011637 ,\n",
       "        0.048679 , -0.065226 ,  0.035772 , -0.14549  , -0.0047683,\n",
       "       -0.2176   ,  0.097211 , -0.23592  ,  0.059933 , -0.043059 ,\n",
       "       -0.142    , -0.13229  ,  0.14831  , -0.10836  , -0.041123 ,\n",
       "       -0.053446 , -0.20878  , -0.056799 ,  0.042364 , -0.073509 ,\n",
       "       -0.021485 ,  0.023052 ,  0.026784 ,  0.01281  ,  0.0029815,\n",
       "        0.074967 ,  0.20921  ,  0.073388 ,  0.034119 , -0.14643  ,\n",
       "       -0.069501 ,  0.14438  ,  0.0901   , -0.030421 ,  0.094469 ,\n",
       "        0.03088  ,  0.014779 , -0.068274 ,  0.1562   , -0.14401  ,\n",
       "       -0.070045 , -0.026215 , -0.028127 , -0.044533 , -0.11838  ,\n",
       "        0.12447  , -0.21847  ,  0.027829 , -0.034787 , -0.1793   ,\n",
       "        0.095557 , -0.0069741,  0.063249 , -0.13063  ,  0.02909  ,\n",
       "       -0.0040399, -0.10868  ,  0.19582  , -0.23576  , -0.23128  ,\n",
       "        0.099063 ,  0.026377 ,  0.096095 ,  0.30016  ,  0.097604 ,\n",
       "        0.019053 ,  0.081196 ,  0.062768 ,  0.23626  , -0.042725 ,\n",
       "       -0.067281 ,  0.1964   , -0.16594  , -0.17855  ,  0.26355  ,\n",
       "       -0.029399 , -0.11128  ,  0.10487  , -0.1206   ,  0.01336  ,\n",
       "       -0.18132  ,  0.0022502,  0.19542  , -0.18097  , -0.30351  ,\n",
       "       -0.10376  ,  0.030874 , -0.040476 , -0.012293 ,  0.042569 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index['de']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we compute the mean and standard variance of this embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb_mean: -0.007466377 emb_std: 0.2691353\n"
     ]
    }
   ],
   "source": [
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "print('emb_mean:',emb_mean,'emb_std:',emb_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we begin to generate some features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embeddings_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-756c343b5331>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfeature_train_es\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_es_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\CIKM AnalytiCup 2018\\src\\feature.py\u001b[0m in \u001b[0;36mget_feature\u001b[1;34m(df, tokenizer)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_vec_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'word2vec_dot'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\CIKM AnalytiCup 2018\\src\\feature.py\u001b[0m in \u001b[0;36mget_vec_feature\u001b[1;34m(df, tokenizer)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0membeddings_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_coefs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrsplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mall_embs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0memb_mean\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0memb_std\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall_embs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_embs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'embeddings_index' is not defined"
     ]
    }
   ],
   "source": [
    "feature_train_es = get_feature(df_es_train, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import gc\n",
    "\n",
    "def lgb_modelfit_nocv(params, dtrain, dvalid, predictors, target='label', objective='binary', metrics='auc',\n",
    "                 feval=None, early_stopping_rounds=50, num_boost_round=3000, verbose_eval=10, categorical_features=None):\n",
    "    lgb_params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': objective,\n",
    "        'metric':metrics,\n",
    "        'learning_rate': 0.04,\n",
    "        #'is_unbalance': 'true',  #because training data is unbalance (replaced with scale_pos_weight)\n",
    "        'num_leaves': 31,  # we should let it be smaller than 2^(max_depth)\n",
    "        'max_depth': -1,  # -1 means no limit\n",
    "        'min_child_samples': 20,  # Minimum number of data need in a child(min_data_in_leaf)\n",
    "        'max_bin': 255,  # Number of bucketed bin for feature values\n",
    "        'subsample': 0.6,  # Subsample ratio of the training instance.\n",
    "        'subsample_freq': 0,  # frequence of subsample, <=0 means no enable\n",
    "        'colsample_bytree': 0.3,  # Subsample ratio of columns when constructing each tree.\n",
    "        'min_child_weight': 5,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
    "        'subsample_for_bin': 200000,  # Number of samples for constructing bin\n",
    "        'min_split_gain': 0,  # lambda_l1, lambda_l2 and min_gain_to_split to regularization\n",
    "        'reg_alpha': 0.99,  # L1 regularization term on weights\n",
    "        'reg_lambda': 0.9,  # L2 regularization term on weights\n",
    "        'nthread': 8,\n",
    "        'verbose': 1,\n",
    "    }\n",
    "\n",
    "    lgb_params.update(params)\n",
    "\n",
    "    print(\"preparing validation datasets\")\n",
    "\n",
    "    xgtrain = lgb.Dataset(dtrain[predictors].values, label=dtrain[target].values,\n",
    "                          feature_name=predictors,\n",
    "                          categorical_feature=categorical_features\n",
    "                          )\n",
    "    xgvalid = lgb.Dataset(dvalid[predictors].values, label=dvalid[target].values,\n",
    "                          feature_name=predictors,\n",
    "                          categorical_feature=categorical_features\n",
    "                          )\n",
    "    del dtrain\n",
    "    del dvalid\n",
    "    gc.collect()\n",
    "\n",
    "    evals_results = {}\n",
    "\n",
    "    bst1 = lgb.train(lgb_params, \n",
    "                     xgtrain, \n",
    "                     valid_sets=[ xgvalid], \n",
    "                     valid_names=['valid'], \n",
    "                     evals_result=evals_results, \n",
    "                     num_boost_round=num_boost_round,\n",
    "                     early_stopping_rounds=early_stopping_rounds,\n",
    "                     verbose_eval=10, \n",
    "                     feval=feval)\n",
    "\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"bst1.best_iteration: \", bst1.best_iteration)\n",
    "    print(metrics+\":\", evals_results['valid'][metrics][bst1.best_iteration-1])\n",
    "\n",
    "    return (bst1,bst1.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_es_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'learning_rate': 0.04,\n",
    "    #'is_unbalance': 'true', # replaced with scale_pos_weight argument\n",
    "    'num_leaves': 31,  # 2^max_depth - 1\n",
    "    'max_depth': -1,  # -1 means no limit\n",
    "    'min_child_samples': 20,  # Minimum number of data need in a child(min_data_in_leaf)\n",
    "    'max_bin': 255,  # Number of bucketed bin for feature values\n",
    "    'subsample': 0.6,  # Subsample ratio of the training instance.\n",
    "    'subsample_freq': 0,  # frequence of subsample, <=0 means no enable\n",
    "    'colsample_bytree': 0.3,  # Subsample ratio of columns when constructing each tree.\n",
    "    'min_child_weight': 5,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
    "    'subsample_for_bin': 200000,  # Number of samples for constructing bin\n",
    "    'min_split_gain': 0,  # lambda_l1, lambda_l2 and min_gain_to_split to regularization\n",
    "    'reg_alpha': 0.99,  # L1 regularization term on weights\n",
    "    'reg_lambda': 0.9,  # L2 regularization term on weights\n",
    "    'scale_pos_weight':200 # because training data is extremely unbalanced \n",
    "}\n",
    "\n",
    "predictors = ['word2vec_dot']\n",
    "\n",
    "(bst,best_iteration) = lgb_modelfit_nocv(params, \n",
    "                        df_es_train[:1200], \n",
    "                        df_es_train[1200:], \n",
    "                        predictors, \n",
    "                        objective='binary', \n",
    "                        metrics='auc',\n",
    "                        early_stopping_rounds=30, \n",
    "                        verbose_eval=True, \n",
    "                        num_boost_round=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['is_attributed'] = bst.predict(df_test[predictors],num_iteration=best_iteration)\n",
    "#     if not debug:\n",
    "#         print(\"writing...\")\n",
    "sub.to_csv('sub_it%d.csv'%(fileno),index=False,float_format='%.9f')\n",
    "print(\"done...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
